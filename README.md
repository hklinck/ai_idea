# Trying to Flesh Out AI Idea

Inspired by: https://vitalik.eth.limo/general/2025/02/28/aihumans.html#1


Currently, we submit a prompt to and receive a response from a singular AI chatbot model (GPT-4o, DeepSeek R1, etc.) 

New Idea: submit a prompt, and a set of scoring functions, to a pool of agents (models, and human-model hybrids) and receive back the ‘Best’ best responses generated by an agent in the pool (determined on the scoring function).
	- This idea should provide users with better (more accurate, cheaper, quicker) responses because you’re leveraging strengths from various models instead of relying on one singular model.


# First Idea:

In the scenario where the prompt is a set of mathematical/accounting/engineering problems, we expect “correct” responses. 

- Define example scoring functions:
    - (Assume we have a correct solutions to a subset of the problems)
    - Scoring function #1: The accuracy of a response on the subset of pre-determined correct solutions
    - Scoring function #2: Cost to generate? -> but the response is already generated, so this doesn’t make sense, unless the agents are competing against each other with the idea that creating responses will be profitable over time.
        - Maybe: AI models just generate a subset of responses and then a model is determined to be the best, and after being chosen to be the best, it generates the rest of the solutions??
            - In this case, the estimated cost can be a aspect of the scoring function.
        - But I guess the user could also just pay more to get a guaranteed better (more accurate) response
    -  Scoring function #3: Time needed to generate responses?
    - Scoring function #4: responses must pass a set of constraints. Ie must be a whole number greater than 34 and less than 57

Agents in the pool all generate and submit their responses (not sure what mechanism for connecting the network? Maybe blockchain)
Then, the responses generated by the pool of agents are scored be the scoring functions, the highest scoring response is determined and returned as the best response to the user
In case of scoring based on the accuracy on a subset of problems. Sampling responses from the model can build our confidence that all of the responses generated by the model are accurate.

How can we find optimal *combinations* of model responses?


Implication of this idea: eventually, agents will develop models which are fine-tuned or tailored specific types of problems, then only submit responses to prompts in their scope of ‘expertise’

# Another Idea:

If you believe there will be a market of multiple models in the future, there will be a need for model experts who consult with people/businesses to determine which model they should be using.

# Second Idea: 

*unrelated to above

Assumption: some models are better (or more efficient) at generating “correct” responses to types of question:

For example, the largest and newest models using Chain-of-thought techniques are the best models for generator responses to complicated reasoning problems.

Meanwhile, smaller models, which are cheaper to for LLM inference (ie. generating responses) are ‘more efficient’ and generate quicker responses for more simple prompts like:  “Name the 3 primary colours”

Proposed Process: Create a model which classifies problems, and then generates a response from the most “best” or “most efficient” model for this type of problem.

- Considerations: there are widely accepted metrics for measuring model accuracy on different types of tasks -> use these rankings in combination with prices?
- This idea seems to get more complicated when we talk about text generation/creative tasks:
    - Look into Gwen Brandon’s article about 

